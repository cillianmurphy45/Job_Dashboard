{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing a Dataset to the Web Project\n",
    "\n",
    "### By Cillian Murphy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on publishing data to the web (LO5).  It will use the data you have captured in project 2 and display it on a web site.Based on the project brief for project 2 shown below, you should have the following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core information you should collect is:\n",
    "- Job Title\n",
    "- Company\n",
    "- Location (City at a minimum)\n",
    "- Date when job was listed\n",
    "\n",
    "Data you should try and obtain where it is included in a job specification:\n",
    "- Salary (or salary range)\n",
    "- Skills required\n",
    "- Years experience required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this you will produce a web site that displays analysis of the data you have captured. Exactly what data and analysis you present depends on the data you have captured. As a guideline, include the following:\n",
    "\n",
    "- Multiple pages, that can be easily navigated.\n",
    "- Charts and/or tables that show aggregate data of current job vacancies.\n",
    "- Charts and/or tables that show historical information about jobs vacancies.\n",
    "- Shows detailed information about current and historical jobs.\n",
    "- Your source data should be stored in a relational database (e.g. MySql).\n",
    "- You may combine database queries with Pandas to perform analysis when preparing data on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.figma.com/design/WKLdJvYYVwWcgRpAb8LKB7/Wireframe_design_CM_CA3?m=auto&t=I6Al2tknzW47bQWs-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataCleaning for current jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, next: 'DataCleaner' = None):  \n",
    "        ''' initialise the next handler reference'''\n",
    "        self.next = next\n",
    "        \n",
    "    def process(self, data):\n",
    "        ''' call process_task on the concrete class\n",
    "        then calls the next handler'''\n",
    "        self.process_task(data)\n",
    "        if self.next is not None:\n",
    "            self.next.process(data)\n",
    "    \n",
    "    def process_task(self, data):\n",
    "        ''' abstract method'''\n",
    "        pass\n",
    "    \n",
    "    def add_datacleaner(self, datacleaner):\n",
    "        '''Adds a new handler to the end of the chain '''\n",
    "        if self.next != None:\n",
    "            self.next.add_datacleaner(datacleaner)\n",
    "        else:\n",
    "            self.next = datacleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData(DataCleaner):\n",
    "    def process_task(self, data):\n",
    "\n",
    "        data[\"salary\"] = data[\"salary\"].str.replace(\"s\\n\",\" \")\n",
    "\n",
    "        # Salary and experience were joined by the circle so had to replace this with a comma\n",
    "        data[\"salary\"] = data[\"salary\"].str.replace(\"â€¢\",\", \")\n",
    "\n",
    "        # Creating the experience column by taking where it had the experience in the salary column and assigning that to experience\n",
    "        data[\"experience\"] = data[\"salary\"].str.extract(r'((?:\\d+\\s*-\\s*\\d+\\s*Years)|(?:Fresher))')\n",
    "\n",
    "        # Finding where it had the salary per month part and assigning that to salary column\n",
    "        data['salary'] = data['salary'].str.extract(r'((?:USD|SGD)\\s[\\d,]+\\s*-\\s*[\\d,]+\\s*per\\s*month)')\n",
    "\n",
    "        # Assigning each experience level to a key value then looping through the experience column to assign the value to each type of experience category\n",
    "        exp_data = {\"Fresher\":1,\"3 - 5 Years\":4,\"5 - 7 Years\":6,\"2 - 4 Years\":3,\"1 - 3 Years\":2,\"2 - 5 Years\":3,\"5 - 8 Years\":6,\"4 - 6 Years\":5,\"3 - 8 Years\":6,\"10 - 12 Years\":11,\"8 - 10 Years\":9,\"1 - 5 Years\":3,\"5 - 10 Years\":7,\"0 - 2 Years\":1}\n",
    "        for exp in exp_data:\n",
    "            data.loc[data[\"experience\"].str.contains(exp,na = False), \"experience\"] = exp_data[exp]\n",
    "\n",
    "        # Extracting what currency the job is listed in and assigning it to a new column currency\n",
    "        data[\"currency\"] = data[\"salary\"].str.extract(r'((?:USD|SGD))')\n",
    "\n",
    "        # Getting rid of any white space in the salary column\n",
    "        data[\"salary\"] = data[\"salary\"].str.strip()\n",
    "\n",
    "        # Gettiong rid of the hyphon between the salaries\n",
    "        data[\"salary\"] = data[\"salary\"].str.replace(\"-\",\" \")\n",
    "\n",
    "        # Getting rid of the USD/SGD currency in the salary column as we have a new column with that info\n",
    "        data['salary'] = data['salary'].str.replace(r'(SGD|USD|\\s*per month)', '', regex=True)\n",
    "\n",
    "        # Splitting the salary column to max and min salaries to compute the median\n",
    "        data[[\"min_salary\",\"max_salary\"]] = data[\"salary\"].str.split(expand=True)\n",
    "\n",
    "        # Getting rid of commas so that I can convert the salaries to floats\n",
    "        data['min_salary'] = data['min_salary'].str.replace(',', '').astype(float)\n",
    "        data['max_salary'] = data['max_salary'].str.replace(',', '').astype(float)\n",
    "\n",
    "        # Computing the median salary then writing that to a new column called median salary\n",
    "        data['median_salary'] = data[['min_salary', 'max_salary']].median(axis=1)\n",
    "\n",
    "        # Drop salary column as have all info neccesary within min, max and median salary\n",
    "        data.drop(columns=[\"salary\"],inplace = True)\n",
    "\n",
    "        # Finding rows where job title was null as the webscraper didnt obtain that data and dropping these rows\n",
    "        indexdata = data[(data['job_title'].isna() == True)].index\n",
    "        data.drop(indexdata,inplace=True)\n",
    "        \n",
    "        return data.to_csv(\"new_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanSkills(DataCleaner):\n",
    "    def process_task(self, data):\n",
    "        skills = pd.DataFrame(data[\"skills\"])\n",
    "\n",
    "        # Getting rid of the /n in between each skill\n",
    "        skills[\"skills\"] = skills[\"skills\"].str.replace(\"\\n\",\", \")\n",
    "\n",
    "        # Splitting the skills every time there is a comma while still being assigned to the job_id\n",
    "        skills = skills.assign(skills=skills['skills'].str.split(',')).explode('skills')\n",
    "        skills['skills'] = skills['skills'].str.strip()\n",
    "\n",
    "        # Dropping null rows\n",
    "        indexdata = skills[(skills['skills'].isna() == True)].index\n",
    "        skills.drop(indexdata,inplace=True)\n",
    "\n",
    "        # dropping skills from original data\n",
    "        data.drop(columns=[\"skills\"],inplace=True)\n",
    "\n",
    "        # Writing the skills data to a csv\n",
    "        return skills.to_csv(\"new_skills.csv\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names for data that will be passed through the data cleaning pipeline\n",
    "data = pd.read_csv(\"current_jobs.csv\")\n",
    "data2 = pd.read_csv(\"table_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up chain calling cleanskills first then passing that data through to the next handler\n",
    "chain = CleanSkills(CleanData())\n",
    "chain.process(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for the cleaned data to pass to my database \n",
    "new_skills_data = pd.read_csv(\"new_skills.csv\")\n",
    "new_jobs_data = pd.read_csv(\"new_jobs.csv\")\n",
    "\n",
    "old_skills_data = pd.read_csv(\"skills_2_2_25.csv\")\n",
    "old_jobs_data = pd.read_csv(\"jobs_2_2_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    # Creating connection to the database\n",
    "    mydb = mysql.connector.connect(user='#', password='#', host='#')\n",
    "\n",
    "    # Creating cursor to interact with my database\n",
    "    cur = mydb.cursor()\n",
    "\n",
    "    cur.execute(\"CREATE DATABASE IF NOT EXISTS Jobs_Data\")\n",
    "\n",
    "\n",
    "    # Connecting to my database\n",
    "    cur.execute(\"USE Jobs_Data\")\n",
    "\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS new_jobs (\n",
    "        job_id INT PRIMARY KEY,\n",
    "        job_title VARCHAR(255),\n",
    "        location VARCHAR(255),\n",
    "        min_salary FLOAT,\n",
    "        median_salary FLOAT,\n",
    "        max_salary FLOAT,\n",
    "        company_name VARCHAR(255),\n",
    "        date_posted VARCHAR(255),\n",
    "        experience FLOAT\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS new_skills (\n",
    "        job_id INT,\n",
    "        skill_name VARCHAR(255),\n",
    "        FOREIGN KEY (job_id) REFERENCES new_jobs(job_id)\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS old_jobs (\n",
    "        job_id INT PRIMARY KEY,\n",
    "        job_title VARCHAR(255),\n",
    "        location VARCHAR(255),\n",
    "        min_salary FLOAT,\n",
    "        median_salary FLOAT,\n",
    "        max_salary FLOAT,\n",
    "        company_name VARCHAR(255),\n",
    "        date_posted VARCHAR(255),\n",
    "        experience FLOAT\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS old_skills (\n",
    "        job_id INT,\n",
    "        skill_name VARCHAR(255),\n",
    "        FOREIGN KEY (job_id) REFERENCES new_jobs(job_id)\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_data(job_data,skills_data):\n",
    "    mydb = mysql.connector.connect(user='#', password='#', host='#',database='Jobs_Data')\n",
    "    cur = mydb.cursor()\n",
    "\n",
    "    # renaming columns to match the sql table\n",
    "    skills_data = skills_data.rename(columns={\"Unnamed: 0\":\"job_id\",\"skills\":\"skill_name\"})\n",
    "\n",
    "    job_data = job_data.rename(columns={\"Unnamed: 0\":\"job_id\"})\n",
    "\n",
    "    # looping through job data and adding the data to my database\n",
    "    for index, row in job_data.iterrows():\n",
    "        cur.execute(\"INSERT INTO new_jobs (job_id,job_title,location,min_salary,median_salary,max_salary,company_name,date_posted,experience) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\",(row[\"job_id\"],row[\"job_title\"],row[\"location\"],row[\"min_salary\"],row[\"median_salary\"],row[\"max_salary\"],row[\"company_name\"],row[\"date_posted\"],row[\"experience\"]))\n",
    "        mydb.commit()\n",
    "\n",
    "    # Filter skills_data to include only rows with job_id present in job_data\n",
    "    valid_job_ids = set(job_data[\"job_id\"])\n",
    "    filtered_skills_data = skills_data[skills_data[\"job_id\"].isin(valid_job_ids)]\n",
    "\n",
    "    for index, row in filtered_skills_data.iterrows():\n",
    "        cur.execute(\"INSERT INTO new_skills (job_id,skill_name) VALUES (%s,%s)\",(row[\"job_id\"],row[\"skill_name\"]))\n",
    "        mydb.commit()\n",
    "\n",
    "    mydb.close()    \n",
    "\n",
    "# passing the csv file into my database\n",
    "insert_new_data(new_jobs_data,new_skills_data)    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_old_data(job_data,skills_data):\n",
    "    mydb = mysql.connector.connect(user='#', password='#', host='#',database='Jobs_Data')\n",
    "    cur = mydb.cursor()\n",
    "\n",
    "    skills_data = skills_data.rename(columns={\"Unnamed: 0\":\"job_id\",\"skills\":\"skill_name\"})\n",
    "\n",
    "    job_data = job_data.rename(columns={\"Unnamed: 0\":\"job_id\"})\n",
    "\n",
    "    for index, row in job_data.iterrows():\n",
    "        cur.execute(\"INSERT INTO old_jobs (job_id,job_title,location,min_salary,median_salary,max_salary,company_name,date_posted,experience) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\",(row[\"job_id\"],row[\"job_title\"],row[\"location\"],row[\"min_salary\"],row[\"median_salary\"],row[\"max_salary\"],row[\"company_name\"],row[\"date_posted\"],row[\"experience\"]))\n",
    "        mydb.commit()\n",
    "\n",
    "    # Filter skills_data to include only rows with job_id present in job_data\n",
    "    valid_job_ids = set(job_data[\"job_id\"])\n",
    "    filtered_skills_data = skills_data[skills_data[\"job_id\"].isin(valid_job_ids)]\n",
    "\n",
    "    for index, row in filtered_skills_data.iterrows():\n",
    "        cur.execute(\"INSERT INTO old_skills (job_id,skill_name) VALUES (%s,%s)\",(row[\"job_id\"],row[\"skill_name\"]))\n",
    "        mydb.commit()\n",
    "\n",
    "    mydb.close()    \n",
    "\n",
    "insert_old_data(old_jobs_data,old_skills_data)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Data for graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Job Vacancy Data \n",
    "- Graph 1 Salary\n",
    "- Graph 2 Jobs Posted per Date\n",
    "- Graph 3 Jobs listed per company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Function to query the data and write to a new csv, by passing in the query to run and the filename to write it to\n",
    "def get_data(query,filename):\n",
    "    try:\n",
    "        # Coneecting to database and setting up my cursor for databse interaction\n",
    "        mydb = mysql.connector.connect(user='#', password='#', host='#',database='Jobs_Data')\n",
    "\n",
    "        cur = mydb.cursor()\n",
    "\n",
    "        # executing the query that has been passed in\n",
    "        cur.execute(query)    \n",
    "\n",
    "        # creating a empty list for writing the results to\n",
    "        data = []\n",
    "\n",
    "        # looping through the results, adding that to the list and finally writing to the csv file\n",
    "        results = cur.fetchall()\n",
    "        for res in results:\n",
    "            data.append(res)\n",
    "        with open(filename,\"w\",newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data)\n",
    "\n",
    "    # Catching any errors in database connection or problems with the query/file\n",
    "    except mysql.connector.Error as er:\n",
    "        print(\"Something went wrong {}\".format(er))\n",
    "\n",
    "    finally:\n",
    "        if mydb is not None and mydb.is_connected():\n",
    "            mydb.close()\n",
    "            print('Database connection closed.')    \n",
    "\n",
    "# Graph 1 Salarys\n",
    "get_data(\"SELECT min_salary,median_salary,max_salary from new_jobs WHERE min_salary is NOT null and median_salary is not null and max_salary is not null ORDER by max_salary desc\",\"salary_data.csv\")\n",
    "\n",
    "# Graph 2 Jobs posted per date\n",
    "get_data(\"SELECT COUNT(job_title) as jobs_per_date,date_posted from new_jobs GROUP by date_posted order by COUNT(job_title) desc\",\"jobs_per_date.csv\")\n",
    "\n",
    "# Graph 3 Jobs listed per company\n",
    "get_data(\"SELECT COUNT(job_title) as jobs_per_company,company_name from new_jobs where company_name is not null GROUP by company_name order by COUNT(job_title) DESC LIMIT 10\",\"jobs_per_company.csv\")\n",
    "\n",
    "# Past Salary Data\n",
    "get_data(\"SELECT min_salary,median_salary,max_salary from old_jobs WHERE min_salary is NOT null and median_salary is not null and max_salary is not null ORDER by max_salary desc\",\"old_jobs_salaries.csv\")\n",
    "\n",
    "# Top 10 current skills\n",
    "get_data(\"SELECT COUNT(skill_name),skill_name from new_skills GROUP by skill_name ORDER by COUNT(skill_name) desc LIMIT 10\",\"top_10_current_skills.csv\")\n",
    "\n",
    "# Top 10 old skills\n",
    "get_data(\"SELECT COUNT(skill_name),skill_name from old_skills GROUP by skill_name ORDER by COUNT(skill_name) desc LIMIT 10\",\"top_10_old_skills.csv\")\n",
    "\n",
    "# Past experience needed\n",
    "get_data(\"SELECT experience,count(experience) as experience_needed from new_jobs where experience is NOT null GROUP by experience ORDER by COUNT(experience) DESC\",\"new_experience_needed.csv\")\n",
    "\n",
    "# Current Experience needed\n",
    "get_data(\"SELECT experience,count(experience) as experience_needed from old_jobs where experience is NOT null GROUP by experience ORDER by COUNT(experience) DESC\",\"old_experience_needed.csv\")\n",
    "\n",
    "# Highest Paying Skills\n",
    "get_data(\"SELECT skill_name, median_salary from new_skills join new_jobs on new_skills.job_id = new_jobs.job_id WHERE median_salary is NOT null GROUP by skill_name ORDER by median_salary DESC LIMIT 25\",\"top_20_paying_skills.csv\")\n",
    "\n",
    "# Companys looking for the most skills\n",
    "get_data(\"SELECT company_name,COUNT(skill_name) as amount_of_skills from new_jobs JOIN new_skills on new_skills.job_id = new_jobs.job_id WHERE company_name is not null GROUP by company_name ORDER by COUNT(skill_name) desc LIMIT 25;\",\"company_skills.csv\")\n",
    "\n",
    "# Different levels of experience\n",
    "get_data(\"SELECT experience,COUNT(experience) as experience_needed from new_jobs GROUP by experience ORDER by experience desc\",\"experience_data.csv\")\n",
    "\n",
    "# Average experience level needed per company\n",
    "get_data(\"SELECT company_name,avg(experience) as average_experience from new_jobs WHERE experience is not null GROUP by company_name ORDER by AVG(experience) desc\",\"avg_exp.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
